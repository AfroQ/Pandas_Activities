{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4 Day 01\n",
    "# Instructor Turn - 01-  Intro To Pandas - üë©‚Äçüè´üßë‚Äçüè´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can create a Pandas Series from a raw list\n",
    "data_series = pd.Series([\"UCLA\", \"UC Berkeley\", \"UC Irvine\",\n",
    "                         \"University of Central Florida\", \"Rutgers University\"])\n",
    "data_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a list of dictionarys into a dataframe\n",
    "states_dicts = [{\"STATE\": \"New Jersey\", \"ABBREVIATION\": \"NJ\"},\n",
    "                {\"STATE\": \"New York\", \"ABBREVIATION\": \"NY\"}]\n",
    "\n",
    "df_states = pd.DataFrame(states_dicts)\n",
    "df_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a single dictionary containing lists into a dataframe\n",
    "df = pd.DataFrame(\n",
    "    {\"Dynasty\": [\"Early Dynastic Period\", \"Old Kingdom\"],\n",
    "     \"Pharoh\": [\"Thinis\", \"Memphis\"]\n",
    "     }\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructor Turn - 02 - Data Functions - üë©‚Äçüè´üßë‚Äçüè´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save path to data set in a variable\n",
    "data_file = \"Resources/dataSet.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Pandas to read data\n",
    "data_file_pd = pd.read_csv(data_file)\n",
    "data_file_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_pd.head() #First five rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a statistical overview of the DataFrame\n",
    "data_file_pd.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference a single column within a DataFrame\n",
    "data_file_pd[\"Amount\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference multiple columns within a DataFrame\n",
    "data_file_pd[[\"Amount\", \"Gender\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The mean method averages the series\n",
    "average = data_file_pd[\"Amount\"].mean()\n",
    "average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sum method adds every entry in the series\n",
    "total = data_file_pd[\"Amount\"].sum()\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The unique method shows every element of the series that appears only once\n",
    "unique = data_file_pd[\"Last Name\"].unique()\n",
    "unique\n",
    "len(unique) #gives list length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The value_counts method counts unique values in a column\n",
    "count = data_file_pd[\"Gender\"].value_counts()\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculations can also be performed on Series and added into DataFrames as new columns\n",
    "thousands_of_dollars = data_file_pd[\"Amount\"]/1000\n",
    "data_file_pd[\"Thousands of Dollars\"] = thousands_of_dollars\n",
    "\n",
    "data_file_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Students Turn - 03 - Training Grounds - üë©‚Äçüéìüë®‚Äçüéì "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Grounds\n",
    "\n",
    "## Instructions\n",
    "\n",
    "* Using the DataFrame provided, perform all of the following actions...\n",
    "\n",
    "* Provide a simple, analytical overview of the dataset's numeric columns\n",
    "\n",
    "* Collect all of the names of the trainers within the dataset\n",
    "\n",
    "* Figure out how many students each trainer has\n",
    "\n",
    "* Find the average weight of the students at the gym\n",
    "\n",
    "* Find the combined weight of all of the students at the gym\n",
    "\n",
    "* Convert the \"Membership (Days)\" column into weeks and then add this new series into the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# A gigantic DataFrame of individuals' names, their trainers, their weight, and their days as gym members\n",
    "training_data = pd.DataFrame({\n",
    "    \"Name\":[\"Gino Walker\",\"Hiedi Wasser\",\"Kerrie Wetzel\",\"Elizabeth Sackett\",\"Jack Mitten\",\"Madalene Wayman\",\"Jamee Horvath\",\"Arlena Reddin\",\"Tula Levan\",\"Teisha Dreier\",\"Leslie Carrier\",\"Arlette Hartson\",\"Romana Merkle\",\"Heath Viviani\",\"Andres Zimmer\",\"Allyson Osman\",\"Yadira Caggiano\",\"Jeanmarie Friedrichs\",\"Leann Ussery\",\"Bee Mom\",\"Pandora Charland\",\"Karena Wooten\",\"Elizabet Albanese\",\"Augusta Borjas\",\"Erma Yadon\",\"Belia Lenser\",\"Karmen Sancho\",\"Edison Mannion\",\"Sonja Hornsby\",\"Morgan Frei\",\"Florencio Murphy\",\"Christoper Hertel\",\"Thalia Stepney\",\"Tarah Argento\",\"Nicol Canfield\",\"Pok Moretti\",\"Barbera Stallings\",\"Muoi Kelso\",\"Cicely Ritz\",\"Sid Demelo\",\"Eura Langan\",\"Vanita An\",\"Frieda Fuhr\",\"Ernest Fitzhenry\",\"Ashlyn Tash\",\"Melodi Mclendon\",\"Rochell Leblanc\",\"Jacqui Reasons\",\"Freeda Mccroy\",\"Vanna Runk\",\"Florinda Milot\",\"Cierra Lecompte\",\"Nancey Kysar\",\"Latasha Dalton\",\"Charlyn Rinaldi\",\"Erline Averett\",\"Mariko Hillary\",\"Rosalyn Trigg\",\"Sherwood Brauer\",\"Hortencia Olesen\",\"Delana Kohut\",\"Geoffrey Mcdade\",\"Iona Delancey\",\"Donnie Read\",\"Cesar Bhatia\",\"Evia Slate\",\"Kaye Hugo\",\"Denise Vento\",\"Lang Kittle\",\"Sherry Whittenberg\",\"Jodi Bracero\",\"Tamera Linneman\",\"Katheryn Koelling\",\"Tonia Shorty\",\"Misha Baxley\",\"Lisbeth Goering\",\"Merle Ladwig\",\"Tammie Omar\",\"Jesusa Avilla\",\"Alda Zabala\",\"Junita Dogan\",\"Jessia Anglin\",\"Peggie Scranton\",\"Dania Clodfelter\",\"Janis Mccarthy\",\"Edmund Galusha\",\"Tonisha Posey\",\"Arvilla Medley\",\"Briana Barbour\",\"Delfina Kiger\",\"Nia Lenig\",\"Ricarda Bulow\",\"Odell Carson\",\"Nydia Clonts\",\"Andree Resendez\",\"Daniela Puma\",\"Sherill Paavola\",\"Gilbert Bloomquist\",\"Shanon Mach\",\"Justin Bangert\",\"Arden Hokanson\",\"Evelyne Bridge\",\"Hee Simek\",\"Ward Deangelis\",\"Jodie Childs\",\"Janis Boehme\",\"Beaulah Glowacki\",\"Denver Stoneham\",\"Tarra Vinton\",\"Deborah Hummell\",\"Ulysses Neil\",\"Kathryn Marques\",\"Rosanna Dake\",\"Gavin Wheat\",\"Tameka Stoke\",\"Janella Clear\",\"Kaye Ciriaco\",\"Suk Bloxham\",\"Gracia Whaley\",\"Philomena Hemingway\",\"Claudette Vaillancourt\",\"Olevia Piche\",\"Trey Chiles\",\"Idalia Scardina\",\"Jenine Tremble\",\"Herbert Krider\",\"Alycia Schrock\",\"Miss Weibel\",\"Pearlene Neidert\",\"Kina Callender\",\"Charlotte Skelley\",\"Theodora Harrigan\",\"Sydney Shreffler\",\"Annamae Trinidad\",\"Tobi Mumme\",\"Rosia Elliot\",\"Debbra Putt\",\"Rena Delosantos\",\"Genna Grennan\",\"Nieves Huf\",\"Berry Lugo\",\"Ayana Verdugo\",\"Joaquin Mazzei\",\"Doris Harmon\",\"Patience Poss\",\"Magaret Zabel\",\"Marylynn Hinojos\",\"Earlene Marcantel\",\"Yuki Evensen\",\"Rema Gay\",\"Delana Haak\",\"Patricia Fetters\",\"Vinnie Elrod\",\"Octavia Bellew\",\"Burma Revard\",\"Lakenya Kato\",\"Vinita Buchner\",\"Sierra Margulies\",\"Shae Funderburg\",\"Jenae Groleau\",\"Louetta Howie\",\"Astrid Duffer\",\"Caron Altizer\",\"Kymberly Amavisca\",\"Mohammad Diedrich\",\"Thora Wrinkle\",\"Bethel Wiemann\",\"Patria Millet\",\"Eldridge Burbach\",\"Alyson Eddie\",\"Zula Hanna\",\"Devin Goodwin\",\"Felipa Kirkwood\",\"Kurtis Kempf\",\"Kasey Lenart\",\"Deena Blankenship\",\"Kandra Wargo\",\"Sherrie Cieslak\",\"Ron Atha\",\"Reggie Barreiro\",\"Daria Saulter\",\"Tandra Eastman\",\"Donnell Lucious\",\"Talisha Rosner\",\"Emiko Bergh\",\"Terresa Launius\",\"Margy Hoobler\",\"Marylou Stelling\",\"Lavonne Justice\",\"Kala Langstaff\",\"China Truett\",\"Louanne Dussault\",\"Thomasena Samaniego\",\"Charlesetta Tarbell\",\"Fatimah Lade\",\"Malisa Cantero\",\"Florencia Litten\",\"Francina Fraise\",\"Patsy London\",\"Deloris Mclaughlin\"],\n",
    "    \"Trainer\":['Bettyann Savory','Mariah Barberio','Gordon Perrine','Pa Dargan','Blanch Victoria','Aldo Byler','Aldo Byler','Williams Camire','Junie Ritenour','Gordon Perrine','Bettyann Savory','Mariah Barberio','Aldo Byler','Barton Stecklein','Bettyann Savory','Barton Stecklein','Gordon Perrine','Pa Dargan','Aldo Byler','Brittani Brin','Bettyann Savory','Phyliss Houk','Bettyann Savory','Junie Ritenour','Aldo Byler','Calvin North','Brittani Brin','Junie Ritenour','Blanch Victoria','Brittani Brin','Bettyann Savory','Blanch Victoria','Mariah Barberio','Bettyann Savory','Blanch Victoria','Brittani Brin','Junie Ritenour','Pa Dargan','Gordon Perrine','Phyliss Houk','Pa Dargan','Mariah Barberio','Phyliss Houk','Phyliss Houk','Calvin North','Williams Camire','Brittani Brin','Gordon Perrine','Bettyann Savory','Bettyann Savory','Pa Dargan','Phyliss Houk','Barton Stecklein','Blanch Victoria','Coleman Dunmire','Phyliss Houk','Blanch Victoria','Pa Dargan','Harland Coolidge','Calvin North','Bettyann Savory','Phyliss Houk','Bettyann Savory','Harland Coolidge','Gordon Perrine','Junie Ritenour','Harland Coolidge','Blanch Victoria','Mariah Barberio','Coleman Dunmire','Aldo Byler','Bettyann Savory','Gordon Perrine','Bettyann Savory','Barton Stecklein','Harland Coolidge','Aldo Byler','Aldo Byler','Pa Dargan','Junie Ritenour','Brittani Brin','Junie Ritenour','Gordon Perrine','Mariah Barberio','Mariah Barberio','Mariah Barberio','Bettyann Savory','Brittani Brin','Aldo Byler','Phyliss Houk','Blanch Victoria','Pa Dargan','Phyliss Houk','Brittani Brin','Barton Stecklein','Coleman Dunmire','Bettyann Savory','Bettyann Savory','Gordon Perrine','Blanch Victoria','Junie Ritenour','Phyliss Houk','Coleman Dunmire','Williams Camire','Harland Coolidge','Williams Camire','Aldo Byler','Harland Coolidge','Gordon Perrine','Brittani Brin','Coleman Dunmire','Calvin North','Phyliss Houk','Brittani Brin','Aldo Byler','Bettyann Savory','Brittani Brin','Gordon Perrine','Calvin North','Harland Coolidge','Coleman Dunmire','Harland Coolidge','Aldo Byler','Junie Ritenour','Blanch Victoria','Harland Coolidge','Blanch Victoria','Junie Ritenour','Harland Coolidge','Junie Ritenour','Gordon Perrine','Brittani Brin','Coleman Dunmire','Williams Camire','Junie Ritenour','Brittani Brin','Calvin North','Barton Stecklein','Barton Stecklein','Mariah Barberio','Coleman Dunmire','Bettyann Savory','Mariah Barberio','Pa Dargan','Barton Stecklein','Coleman Dunmire','Brittani Brin','Barton Stecklein','Pa Dargan','Barton Stecklein','Junie Ritenour','Bettyann Savory','Williams Camire','Pa Dargan','Calvin North','Williams Camire','Coleman Dunmire','Aldo Byler','Barton Stecklein','Coleman Dunmire','Blanch Victoria','Mariah Barberio','Mariah Barberio','Harland Coolidge','Barton Stecklein','Phyliss Houk','Pa Dargan','Bettyann Savory','Barton Stecklein','Harland Coolidge','Junie Ritenour','Pa Dargan','Mariah Barberio','Blanch Victoria','Williams Camire','Phyliss Houk','Phyliss Houk','Coleman Dunmire','Mariah Barberio','Gordon Perrine','Coleman Dunmire','Brittani Brin','Pa Dargan','Coleman Dunmire','Brittani Brin','Blanch Victoria','Coleman Dunmire','Gordon Perrine','Coleman Dunmire','Aldo Byler','Aldo Byler','Mariah Barberio','Williams Camire','Phyliss Houk','Aldo Byler','Williams Camire','Aldo Byler','Williams Camire','Coleman Dunmire','Phyliss Houk'],\n",
    "    \"Weight\":[128,180,193,177,237,166,224,208,177,241,114,161,162,151,220,142,193,193,124,130,132,141,190,239,213,131,172,127,184,157,215,122,181,240,218,205,239,217,234,158,180,131,194,171,177,110,117,114,217,123,248,189,198,127,182,121,224,111,151,170,188,150,137,231,222,186,139,175,178,246,150,154,129,216,144,198,228,183,173,129,157,199,186,232,172,157,246,239,214,161,132,208,187,224,164,177,175,224,219,235,112,241,243,179,208,196,131,207,182,233,191,162,173,197,190,182,231,196,196,143,250,174,138,135,164,204,235,192,114,179,215,127,185,213,250,213,153,217,176,190,119,167,118,208,113,206,200,236,159,218,168,159,156,183,121,203,215,209,179,219,174,220,129,188,217,250,166,157,112,236,182,144,189,243,238,147,165,115,160,134,245,174,238,157,150,184,174,134,134,248,199,165,117,119,162,112,170,224,247,217],\n",
    "    \"Membership (Days)\":[52,70,148,124,186,157,127,155,37,185,158,129,93,69,124,13,76,153,164,161,48,121,167,69,39,163,7,34,176,169,108,162,195,86,155,77,197,200,80,142,179,67,58,145,188,147,125,15,13,173,125,4,61,29,132,110,62,137,197,135,162,174,32,151,149,65,18,42,63,62,104,200,189,40,38,199,1,12,8,2,195,30,7,72,130,144,2,34,200,143,43,196,22,115,171,54,143,59,14,52,109,115,187,185,26,19,178,18,120,169,45,52,130,69,168,178,96,22,78,152,39,51,118,130,60,156,108,69,103,158,165,142,86,91,117,77,57,169,86,188,97,111,22,83,81,177,163,35,12,164,21,181,171,138,22,107,58,51,38,128,19,193,157,13,104,89,13,10,26,190,179,101,7,159,100,49,120,109,56,199,51,108,47,171,69,162,74,119,148,88,32,159,65,146,140,171,88,18,59,13]\n",
    "})\n",
    "training_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting a summary of all numeric data\n",
    "training_data.describe()\n",
    "\n",
    "# Finding the names of the trainers\n",
    "print(training_data[\"Trainer\"].unique())\n",
    "\n",
    "# Finding how many students each trainer has\n",
    "stu_count = training_data[\"Trainer\"].value_counts()\n",
    "stu_count\n",
    "\n",
    "# Finding the average weight of all students\n",
    "avg_weight = training_data[\"Weight\"].mean()\n",
    "avg_weight\n",
    "\n",
    "# Finding the combined weight of all students\n",
    "sum_weight = training_data[\"Weight\"].sum()\n",
    "sum_weight\n",
    "\n",
    "# Converting the membership days into weeks and then adding a column to the DataFrame\n",
    "weeks = training_data[\"Membership (Days)\"]/7\n",
    "training_data[\"Membership (Weeks)\"] = weeks\n",
    "training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><strong>Activity 03 Solution ‚úÖ</strong></summary>\n",
    "    \n",
    "```python\n",
    "# Import Dependencies\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# A gigantic DataFrame of individuals' names, their trainers, their weight, and their days as gym members\n",
    "training_data = pd.DataFrame({\n",
    "    \"Name\":[\"Gino Walker\",\"Hiedi Wasser\",\"Kerrie Wetzel\",\"Elizabeth Sackett\",\"Jack Mitten\",\"Madalene Wayman\",\"Jamee Horvath\",\"Arlena Reddin\",\"Tula Levan\",\"Teisha Dreier\",\"Leslie Carrier\",\"Arlette Hartson\",\"Romana Merkle\",\"Heath Viviani\",\"Andres Zimmer\",\"Allyson Osman\",\"Yadira Caggiano\",\"Jeanmarie Friedrichs\",\"Leann Ussery\",\"Bee Mom\",\"Pandora Charland\",\"Karena Wooten\",\"Elizabet Albanese\",\"Augusta Borjas\",\"Erma Yadon\",\"Belia Lenser\",\"Karmen Sancho\",\"Edison Mannion\",\"Sonja Hornsby\",\"Morgan Frei\",\"Florencio Murphy\",\"Christoper Hertel\",\"Thalia Stepney\",\"Tarah Argento\",\"Nicol Canfield\",\"Pok Moretti\",\"Barbera Stallings\",\"Muoi Kelso\",\"Cicely Ritz\",\"Sid Demelo\",\"Eura Langan\",\"Vanita An\",\"Frieda Fuhr\",\"Ernest Fitzhenry\",\"Ashlyn Tash\",\"Melodi Mclendon\",\"Rochell Leblanc\",\"Jacqui Reasons\",\"Freeda Mccroy\",\"Vanna Runk\",\"Florinda Milot\",\"Cierra Lecompte\",\"Nancey Kysar\",\"Latasha Dalton\",\"Charlyn Rinaldi\",\"Erline Averett\",\"Mariko Hillary\",\"Rosalyn Trigg\",\"Sherwood Brauer\",\"Hortencia Olesen\",\"Delana Kohut\",\"Geoffrey Mcdade\",\"Iona Delancey\",\"Donnie Read\",\"Cesar Bhatia\",\"Evia Slate\",\"Kaye Hugo\",\"Denise Vento\",\"Lang Kittle\",\"Sherry Whittenberg\",\"Jodi Bracero\",\"Tamera Linneman\",\"Katheryn Koelling\",\"Tonia Shorty\",\"Misha Baxley\",\"Lisbeth Goering\",\"Merle Ladwig\",\"Tammie Omar\",\"Jesusa Avilla\",\"Alda Zabala\",\"Junita Dogan\",\"Jessia Anglin\",\"Peggie Scranton\",\"Dania Clodfelter\",\"Janis Mccarthy\",\"Edmund Galusha\",\"Tonisha Posey\",\"Arvilla Medley\",\"Briana Barbour\",\"Delfina Kiger\",\"Nia Lenig\",\"Ricarda Bulow\",\"Odell Carson\",\"Nydia Clonts\",\"Andree Resendez\",\"Daniela Puma\",\"Sherill Paavola\",\"Gilbert Bloomquist\",\"Shanon Mach\",\"Justin Bangert\",\"Arden Hokanson\",\"Evelyne Bridge\",\"Hee Simek\",\"Ward Deangelis\",\"Jodie Childs\",\"Janis Boehme\",\"Beaulah Glowacki\",\"Denver Stoneham\",\"Tarra Vinton\",\"Deborah Hummell\",\"Ulysses Neil\",\"Kathryn Marques\",\"Rosanna Dake\",\"Gavin Wheat\",\"Tameka Stoke\",\"Janella Clear\",\"Kaye Ciriaco\",\"Suk Bloxham\",\"Gracia Whaley\",\"Philomena Hemingway\",\"Claudette Vaillancourt\",\"Olevia Piche\",\"Trey Chiles\",\"Idalia Scardina\",\"Jenine Tremble\",\"Herbert Krider\",\"Alycia Schrock\",\"Miss Weibel\",\"Pearlene Neidert\",\"Kina Callender\",\"Charlotte Skelley\",\"Theodora Harrigan\",\"Sydney Shreffler\",\"Annamae Trinidad\",\"Tobi Mumme\",\"Rosia Elliot\",\"Debbra Putt\",\"Rena Delosantos\",\"Genna Grennan\",\"Nieves Huf\",\"Berry Lugo\",\"Ayana Verdugo\",\"Joaquin Mazzei\",\"Doris Harmon\",\"Patience Poss\",\"Magaret Zabel\",\"Marylynn Hinojos\",\"Earlene Marcantel\",\"Yuki Evensen\",\"Rema Gay\",\"Delana Haak\",\"Patricia Fetters\",\"Vinnie Elrod\",\"Octavia Bellew\",\"Burma Revard\",\"Lakenya Kato\",\"Vinita Buchner\",\"Sierra Margulies\",\"Shae Funderburg\",\"Jenae Groleau\",\"Louetta Howie\",\"Astrid Duffer\",\"Caron Altizer\",\"Kymberly Amavisca\",\"Mohammad Diedrich\",\"Thora Wrinkle\",\"Bethel Wiemann\",\"Patria Millet\",\"Eldridge Burbach\",\"Alyson Eddie\",\"Zula Hanna\",\"Devin Goodwin\",\"Felipa Kirkwood\",\"Kurtis Kempf\",\"Kasey Lenart\",\"Deena Blankenship\",\"Kandra Wargo\",\"Sherrie Cieslak\",\"Ron Atha\",\"Reggie Barreiro\",\"Daria Saulter\",\"Tandra Eastman\",\"Donnell Lucious\",\"Talisha Rosner\",\"Emiko Bergh\",\"Terresa Launius\",\"Margy Hoobler\",\"Marylou Stelling\",\"Lavonne Justice\",\"Kala Langstaff\",\"China Truett\",\"Louanne Dussault\",\"Thomasena Samaniego\",\"Charlesetta Tarbell\",\"Fatimah Lade\",\"Malisa Cantero\",\"Florencia Litten\",\"Francina Fraise\",\"Patsy London\",\"Deloris Mclaughlin\"],\n",
    "    \"Trainer\":['Bettyann Savory','Mariah Barberio','Gordon Perrine','Pa Dargan','Blanch Victoria','Aldo Byler','Aldo Byler','Williams Camire','Junie Ritenour','Gordon Perrine','Bettyann Savory','Mariah Barberio','Aldo Byler','Barton Stecklein','Bettyann Savory','Barton Stecklein','Gordon Perrine','Pa Dargan','Aldo Byler','Brittani Brin','Bettyann Savory','Phyliss Houk','Bettyann Savory','Junie Ritenour','Aldo Byler','Calvin North','Brittani Brin','Junie Ritenour','Blanch Victoria','Brittani Brin','Bettyann Savory','Blanch Victoria','Mariah Barberio','Bettyann Savory','Blanch Victoria','Brittani Brin','Junie Ritenour','Pa Dargan','Gordon Perrine','Phyliss Houk','Pa Dargan','Mariah Barberio','Phyliss Houk','Phyliss Houk','Calvin North','Williams Camire','Brittani Brin','Gordon Perrine','Bettyann Savory','Bettyann Savory','Pa Dargan','Phyliss Houk','Barton Stecklein','Blanch Victoria','Coleman Dunmire','Phyliss Houk','Blanch Victoria','Pa Dargan','Harland Coolidge','Calvin North','Bettyann Savory','Phyliss Houk','Bettyann Savory','Harland Coolidge','Gordon Perrine','Junie Ritenour','Harland Coolidge','Blanch Victoria','Mariah Barberio','Coleman Dunmire','Aldo Byler','Bettyann Savory','Gordon Perrine','Bettyann Savory','Barton Stecklein','Harland Coolidge','Aldo Byler','Aldo Byler','Pa Dargan','Junie Ritenour','Brittani Brin','Junie Ritenour','Gordon Perrine','Mariah Barberio','Mariah Barberio','Mariah Barberio','Bettyann Savory','Brittani Brin','Aldo Byler','Phyliss Houk','Blanch Victoria','Pa Dargan','Phyliss Houk','Brittani Brin','Barton Stecklein','Coleman Dunmire','Bettyann Savory','Bettyann Savory','Gordon Perrine','Blanch Victoria','Junie Ritenour','Phyliss Houk','Coleman Dunmire','Williams Camire','Harland Coolidge','Williams Camire','Aldo Byler','Harland Coolidge','Gordon Perrine','Brittani Brin','Coleman Dunmire','Calvin North','Phyliss Houk','Brittani Brin','Aldo Byler','Bettyann Savory','Brittani Brin','Gordon Perrine','Calvin North','Harland Coolidge','Coleman Dunmire','Harland Coolidge','Aldo Byler','Junie Ritenour','Blanch Victoria','Harland Coolidge','Blanch Victoria','Junie Ritenour','Harland Coolidge','Junie Ritenour','Gordon Perrine','Brittani Brin','Coleman Dunmire','Williams Camire','Junie Ritenour','Brittani Brin','Calvin North','Barton Stecklein','Barton Stecklein','Mariah Barberio','Coleman Dunmire','Bettyann Savory','Mariah Barberio','Pa Dargan','Barton Stecklein','Coleman Dunmire','Brittani Brin','Barton Stecklein','Pa Dargan','Barton Stecklein','Junie Ritenour','Bettyann Savory','Williams Camire','Pa Dargan','Calvin North','Williams Camire','Coleman Dunmire','Aldo Byler','Barton Stecklein','Coleman Dunmire','Blanch Victoria','Mariah Barberio','Mariah Barberio','Harland Coolidge','Barton Stecklein','Phyliss Houk','Pa Dargan','Bettyann Savory','Barton Stecklein','Harland Coolidge','Junie Ritenour','Pa Dargan','Mariah Barberio','Blanch Victoria','Williams Camire','Phyliss Houk','Phyliss Houk','Coleman Dunmire','Mariah Barberio','Gordon Perrine','Coleman Dunmire','Brittani Brin','Pa Dargan','Coleman Dunmire','Brittani Brin','Blanch Victoria','Coleman Dunmire','Gordon Perrine','Coleman Dunmire','Aldo Byler','Aldo Byler','Mariah Barberio','Williams Camire','Phyliss Houk','Aldo Byler','Williams Camire','Aldo Byler','Williams Camire','Coleman Dunmire','Phyliss Houk'],\n",
    "    \"Weight\":[128,180,193,177,237,166,224,208,177,241,114,161,162,151,220,142,193,193,124,130,132,141,190,239,213,131,172,127,184,157,215,122,181,240,218,205,239,217,234,158,180,131,194,171,177,110,117,114,217,123,248,189,198,127,182,121,224,111,151,170,188,150,137,231,222,186,139,175,178,246,150,154,129,216,144,198,228,183,173,129,157,199,186,232,172,157,246,239,214,161,132,208,187,224,164,177,175,224,219,235,112,241,243,179,208,196,131,207,182,233,191,162,173,197,190,182,231,196,196,143,250,174,138,135,164,204,235,192,114,179,215,127,185,213,250,213,153,217,176,190,119,167,118,208,113,206,200,236,159,218,168,159,156,183,121,203,215,209,179,219,174,220,129,188,217,250,166,157,112,236,182,144,189,243,238,147,165,115,160,134,245,174,238,157,150,184,174,134,134,248,199,165,117,119,162,112,170,224,247,217],\n",
    "    \"Membership (Days)\":[52,70,148,124,186,157,127,155,37,185,158,129,93,69,124,13,76,153,164,161,48,121,167,69,39,163,7,34,176,169,108,162,195,86,155,77,197,200,80,142,179,67,58,145,188,147,125,15,13,173,125,4,61,29,132,110,62,137,197,135,162,174,32,151,149,65,18,42,63,62,104,200,189,40,38,199,1,12,8,2,195,30,7,72,130,144,2,34,200,143,43,196,22,115,171,54,143,59,14,52,109,115,187,185,26,19,178,18,120,169,45,52,130,69,168,178,96,22,78,152,39,51,118,130,60,156,108,69,103,158,165,142,86,91,117,77,57,169,86,188,97,111,22,83,81,177,163,35,12,164,21,181,171,138,22,107,58,51,38,128,19,193,157,13,104,89,13,10,26,190,179,101,7,159,100,49,120,109,56,199,51,108,47,171,69,162,74,119,148,88,32,159,65,146,140,171,88,18,59,13]\n",
    "})\n",
    "training_data.head(10)\n",
    "\n",
    "# Collecting a summary of all numeric data\n",
    "training_data.describe()\n",
    "# Finding the names of the trainers\n",
    "training_data[\"Trainer\"].unique()\n",
    "# Finding how many students each trainer has\n",
    "training_data[\"Trainer\"].value_counts()\n",
    "# Finding the average weight of all students\n",
    "training_data[\"Weight\"].mean()\n",
    "# Finding the combined weight of all students\n",
    "training_data[\"Weight\"].sum()\n",
    "# Converting the membership days into weeks and then adding a column to the DataFrame\n",
    "\n",
    "weeks = training_data[\"Membership (Days)\"]/7\n",
    "training_data[\"Membership (Weeks)\"] = weeks\n",
    "\n",
    "training_data.head()\n",
    "\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructor Turn - 04 - Reading Writing CSV - üë©‚Äçüè´üßë‚Äçüè´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Store filepath in a variable\n",
    "file_one = \"Resources/DataOne.csv\"\n",
    "\n",
    "# Read our Data file with the pandas library\n",
    "# Not every CSV requires an encoding, but be aware this can come up\n",
    "file_one_df = pd.read_csv(file_one, encoding=\"ISO-8859-1\")\n",
    "\n",
    "# Show just the header\n",
    "file_one_df.head()\n",
    "\n",
    "# Show a single column\n",
    "file_one_df[\"first_name\"].head()\n",
    "\n",
    "# Show mulitple specific columns--note the extra brackets\n",
    "file_one_df[[\"first_name\", \"email\"]].head()\n",
    "\n",
    "# Head does not change the DataFrame--it only displays it\n",
    "file_one_df.head()\n",
    "\n",
    "# Export file as a CSV, without the Pandas index, but with the header\n",
    "file_one_df.to_csv(\"output/fileOne.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Students Turn - 05 - Good Reads - üë©‚Äçüéìüë®‚Äçüéì"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GoodReads\n",
    "\n",
    "## Instructions\n",
    "\n",
    "* Read in the GoodReads CSV using Pandas with utf-8 encoding.\n",
    "\n",
    "* Get the date types of each column.\n",
    "\n",
    "* Get a list of all columns within the DataFrame.\n",
    "\n",
    "* Remove unnecessary columns from the DataFrame so that only the following columns remain: `isbn`, `original_publication_year`, `original_title`, `authors`, `ratings_1`, `ratings_2`, `ratings_3`, `ratings_4`, and `ratings_5`\n",
    "\n",
    "* Rename the columns to the following: `ISBN`, `Publication Year`, `Original Title`, `Authors`, `One Star Reviews`, `Two Star Reviews`, `Three Star Reviews`, `Four Star Reviews`, and `Five Star Reviews`\n",
    "\n",
    "## Hints\n",
    "\n",
    "* The base CSV file uses UTF-8 encoding. Trying to read in the file using some other kind of encoding could lead to strange characters appearing within the dataset.\n",
    "\n",
    "Data Source: [Goodreads Dataset](http://fastml.com/goodbooks-10k-a-new-dataset-for-book-recommendations/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>book_id</th>\n",
       "      <th>best_book_id</th>\n",
       "      <th>work_id</th>\n",
       "      <th>books_count</th>\n",
       "      <th>isbn</th>\n",
       "      <th>isbn13</th>\n",
       "      <th>authors</th>\n",
       "      <th>original_publication_year</th>\n",
       "      <th>original_title</th>\n",
       "      <th>...</th>\n",
       "      <th>ratings_count</th>\n",
       "      <th>work_ratings_count</th>\n",
       "      <th>work_text_reviews_count</th>\n",
       "      <th>ratings_1</th>\n",
       "      <th>ratings_2</th>\n",
       "      <th>ratings_3</th>\n",
       "      <th>ratings_4</th>\n",
       "      <th>ratings_5</th>\n",
       "      <th>image_url</th>\n",
       "      <th>small_image_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2767052</td>\n",
       "      <td>2767052</td>\n",
       "      <td>2792775</td>\n",
       "      <td>272</td>\n",
       "      <td>439023483</td>\n",
       "      <td>9.780439e+12</td>\n",
       "      <td>Suzanne Collins</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>The Hunger Games</td>\n",
       "      <td>...</td>\n",
       "      <td>4780653</td>\n",
       "      <td>4942365</td>\n",
       "      <td>155254</td>\n",
       "      <td>66715</td>\n",
       "      <td>127936</td>\n",
       "      <td>560092</td>\n",
       "      <td>1481305</td>\n",
       "      <td>2706317</td>\n",
       "      <td>https://images.gr-assets.com/books/1447303603m...</td>\n",
       "      <td>https://images.gr-assets.com/books/1447303603s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4640799</td>\n",
       "      <td>491</td>\n",
       "      <td>439554934</td>\n",
       "      <td>9.780440e+12</td>\n",
       "      <td>J.K. Rowling, Mary GrandPr√©</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>Harry Potter and the Philosopher's Stone</td>\n",
       "      <td>...</td>\n",
       "      <td>4602479</td>\n",
       "      <td>4800065</td>\n",
       "      <td>75867</td>\n",
       "      <td>75504</td>\n",
       "      <td>101676</td>\n",
       "      <td>455024</td>\n",
       "      <td>1156318</td>\n",
       "      <td>3011543</td>\n",
       "      <td>https://images.gr-assets.com/books/1474154022m...</td>\n",
       "      <td>https://images.gr-assets.com/books/1474154022s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>41865</td>\n",
       "      <td>41865</td>\n",
       "      <td>3212258</td>\n",
       "      <td>226</td>\n",
       "      <td>316015849</td>\n",
       "      <td>9.780316e+12</td>\n",
       "      <td>Stephenie Meyer</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>Twilight</td>\n",
       "      <td>...</td>\n",
       "      <td>3866839</td>\n",
       "      <td>3916824</td>\n",
       "      <td>95009</td>\n",
       "      <td>456191</td>\n",
       "      <td>436802</td>\n",
       "      <td>793319</td>\n",
       "      <td>875073</td>\n",
       "      <td>1355439</td>\n",
       "      <td>https://images.gr-assets.com/books/1361039443m...</td>\n",
       "      <td>https://images.gr-assets.com/books/1361039443s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2657</td>\n",
       "      <td>2657</td>\n",
       "      <td>3275794</td>\n",
       "      <td>487</td>\n",
       "      <td>61120081</td>\n",
       "      <td>9.780061e+12</td>\n",
       "      <td>Harper Lee</td>\n",
       "      <td>1960.0</td>\n",
       "      <td>To Kill a Mockingbird</td>\n",
       "      <td>...</td>\n",
       "      <td>3198671</td>\n",
       "      <td>3340896</td>\n",
       "      <td>72586</td>\n",
       "      <td>60427</td>\n",
       "      <td>117415</td>\n",
       "      <td>446835</td>\n",
       "      <td>1001952</td>\n",
       "      <td>1714267</td>\n",
       "      <td>https://images.gr-assets.com/books/1361975680m...</td>\n",
       "      <td>https://images.gr-assets.com/books/1361975680s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>4671</td>\n",
       "      <td>4671</td>\n",
       "      <td>245494</td>\n",
       "      <td>1356</td>\n",
       "      <td>743273567</td>\n",
       "      <td>9.780743e+12</td>\n",
       "      <td>F. Scott Fitzgerald</td>\n",
       "      <td>1925.0</td>\n",
       "      <td>The Great Gatsby</td>\n",
       "      <td>...</td>\n",
       "      <td>2683664</td>\n",
       "      <td>2773745</td>\n",
       "      <td>51992</td>\n",
       "      <td>86236</td>\n",
       "      <td>197621</td>\n",
       "      <td>606158</td>\n",
       "      <td>936012</td>\n",
       "      <td>947718</td>\n",
       "      <td>https://images.gr-assets.com/books/1490528560m...</td>\n",
       "      <td>https://images.gr-assets.com/books/1490528560s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  book_id  best_book_id  work_id  books_count       isbn        isbn13  \\\n",
       "0   1  2767052       2767052  2792775          272  439023483  9.780439e+12   \n",
       "1   2        3             3  4640799          491  439554934  9.780440e+12   \n",
       "2   3    41865         41865  3212258          226  316015849  9.780316e+12   \n",
       "3   4     2657          2657  3275794          487   61120081  9.780061e+12   \n",
       "4   5     4671          4671   245494         1356  743273567  9.780743e+12   \n",
       "\n",
       "                       authors  original_publication_year  \\\n",
       "0              Suzanne Collins                     2008.0   \n",
       "1  J.K. Rowling, Mary GrandPr√©                     1997.0   \n",
       "2              Stephenie Meyer                     2005.0   \n",
       "3                   Harper Lee                     1960.0   \n",
       "4          F. Scott Fitzgerald                     1925.0   \n",
       "\n",
       "                             original_title  ... ratings_count  \\\n",
       "0                          The Hunger Games  ...       4780653   \n",
       "1  Harry Potter and the Philosopher's Stone  ...       4602479   \n",
       "2                                  Twilight  ...       3866839   \n",
       "3                     To Kill a Mockingbird  ...       3198671   \n",
       "4                          The Great Gatsby  ...       2683664   \n",
       "\n",
       "  work_ratings_count  work_text_reviews_count  ratings_1  ratings_2  \\\n",
       "0            4942365                   155254      66715     127936   \n",
       "1            4800065                    75867      75504     101676   \n",
       "2            3916824                    95009     456191     436802   \n",
       "3            3340896                    72586      60427     117415   \n",
       "4            2773745                    51992      86236     197621   \n",
       "\n",
       "   ratings_3  ratings_4  ratings_5  \\\n",
       "0     560092    1481305    2706317   \n",
       "1     455024    1156318    3011543   \n",
       "2     793319     875073    1355439   \n",
       "3     446835    1001952    1714267   \n",
       "4     606158     936012     947718   \n",
       "\n",
       "                                           image_url  \\\n",
       "0  https://images.gr-assets.com/books/1447303603m...   \n",
       "1  https://images.gr-assets.com/books/1474154022m...   \n",
       "2  https://images.gr-assets.com/books/1361039443m...   \n",
       "3  https://images.gr-assets.com/books/1361975680m...   \n",
       "4  https://images.gr-assets.com/books/1490528560m...   \n",
       "\n",
       "                                     small_image_url  \n",
       "0  https://images.gr-assets.com/books/1447303603s...  \n",
       "1  https://images.gr-assets.com/books/1474154022s...  \n",
       "2  https://images.gr-assets.com/books/1361039443s...  \n",
       "3  https://images.gr-assets.com/books/1361975680s...  \n",
       "4  https://images.gr-assets.com/books/1490528560s...  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Make a reference to the books.csv file path\n",
    "csv_path = \"Resources/books.csv\"\n",
    "\n",
    "# Import the books.csv file as a DataFrame\n",
    "books_df = pd.read_csv(csv_path, encoding=\"utf-8\")\n",
    "books_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isbn</th>\n",
       "      <th>original_publication_year</th>\n",
       "      <th>original_title</th>\n",
       "      <th>authors</th>\n",
       "      <th>ratings_1</th>\n",
       "      <th>ratings_2</th>\n",
       "      <th>ratings_3</th>\n",
       "      <th>ratings_4</th>\n",
       "      <th>ratings_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>439023483</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>The Hunger Games</td>\n",
       "      <td>Suzanne Collins</td>\n",
       "      <td>66715</td>\n",
       "      <td>127936</td>\n",
       "      <td>560092</td>\n",
       "      <td>1481305</td>\n",
       "      <td>2706317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>439554934</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>Harry Potter and the Philosopher's Stone</td>\n",
       "      <td>J.K. Rowling, Mary GrandPr√©</td>\n",
       "      <td>75504</td>\n",
       "      <td>101676</td>\n",
       "      <td>455024</td>\n",
       "      <td>1156318</td>\n",
       "      <td>3011543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>316015849</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>Twilight</td>\n",
       "      <td>Stephenie Meyer</td>\n",
       "      <td>456191</td>\n",
       "      <td>436802</td>\n",
       "      <td>793319</td>\n",
       "      <td>875073</td>\n",
       "      <td>1355439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61120081</td>\n",
       "      <td>1960.0</td>\n",
       "      <td>To Kill a Mockingbird</td>\n",
       "      <td>Harper Lee</td>\n",
       "      <td>60427</td>\n",
       "      <td>117415</td>\n",
       "      <td>446835</td>\n",
       "      <td>1001952</td>\n",
       "      <td>1714267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>743273567</td>\n",
       "      <td>1925.0</td>\n",
       "      <td>The Great Gatsby</td>\n",
       "      <td>F. Scott Fitzgerald</td>\n",
       "      <td>86236</td>\n",
       "      <td>197621</td>\n",
       "      <td>606158</td>\n",
       "      <td>936012</td>\n",
       "      <td>947718</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        isbn  original_publication_year  \\\n",
       "0  439023483                     2008.0   \n",
       "1  439554934                     1997.0   \n",
       "2  316015849                     2005.0   \n",
       "3   61120081                     1960.0   \n",
       "4  743273567                     1925.0   \n",
       "\n",
       "                             original_title                      authors  \\\n",
       "0                          The Hunger Games              Suzanne Collins   \n",
       "1  Harry Potter and the Philosopher's Stone  J.K. Rowling, Mary GrandPr√©   \n",
       "2                                  Twilight              Stephenie Meyer   \n",
       "3                     To Kill a Mockingbird                   Harper Lee   \n",
       "4                          The Great Gatsby          F. Scott Fitzgerald   \n",
       "\n",
       "   ratings_1  ratings_2  ratings_3  ratings_4  ratings_5  \n",
       "0      66715     127936     560092    1481305    2706317  \n",
       "1      75504     101676     455024    1156318    3011543  \n",
       "2     456191     436802     793319     875073    1355439  \n",
       "3      60427     117415     446835    1001952    1714267  \n",
       "4      86236     197621     606158     936012     947718  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove unecessary columns from the DataFrame and save the new DataFrame\n",
    "# Only keep: \"isbn\", \"original_publication_year\", \"original_title\", \"authors\",\n",
    "# \"ratings_1\", \"ratings_2\", \"ratings_3\", \"ratings_4\", \"ratings_5\"\n",
    "new_book_df = books_df[[\"isbn\", \"original_publication_year\", \"original_title\", \"authors\",\n",
    " \"ratings_1\", \"ratings_2\", \"ratings_3\", \"ratings_4\", \"ratings_5\"]]\n",
    "new_book_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Publication Year</th>\n",
       "      <th>Original Title</th>\n",
       "      <th>Authors</th>\n",
       "      <th>One Star Reviews</th>\n",
       "      <th>Two Star Reviews</th>\n",
       "      <th>Three Star Reviews</th>\n",
       "      <th>Four Star Reviews</th>\n",
       "      <th>Five Star Reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>439023483</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>The Hunger Games</td>\n",
       "      <td>Suzanne Collins</td>\n",
       "      <td>66715</td>\n",
       "      <td>127936</td>\n",
       "      <td>560092</td>\n",
       "      <td>1481305</td>\n",
       "      <td>2706317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>439554934</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>Harry Potter and the Philosopher's Stone</td>\n",
       "      <td>J.K. Rowling, Mary GrandPr√©</td>\n",
       "      <td>75504</td>\n",
       "      <td>101676</td>\n",
       "      <td>455024</td>\n",
       "      <td>1156318</td>\n",
       "      <td>3011543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>316015849</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>Twilight</td>\n",
       "      <td>Stephenie Meyer</td>\n",
       "      <td>456191</td>\n",
       "      <td>436802</td>\n",
       "      <td>793319</td>\n",
       "      <td>875073</td>\n",
       "      <td>1355439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61120081</td>\n",
       "      <td>1960.0</td>\n",
       "      <td>To Kill a Mockingbird</td>\n",
       "      <td>Harper Lee</td>\n",
       "      <td>60427</td>\n",
       "      <td>117415</td>\n",
       "      <td>446835</td>\n",
       "      <td>1001952</td>\n",
       "      <td>1714267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>743273567</td>\n",
       "      <td>1925.0</td>\n",
       "      <td>The Great Gatsby</td>\n",
       "      <td>F. Scott Fitzgerald</td>\n",
       "      <td>86236</td>\n",
       "      <td>197621</td>\n",
       "      <td>606158</td>\n",
       "      <td>936012</td>\n",
       "      <td>947718</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ISBN  Publication Year                            Original Title  \\\n",
       "0  439023483            2008.0                          The Hunger Games   \n",
       "1  439554934            1997.0  Harry Potter and the Philosopher's Stone   \n",
       "2  316015849            2005.0                                  Twilight   \n",
       "3   61120081            1960.0                     To Kill a Mockingbird   \n",
       "4  743273567            1925.0                          The Great Gatsby   \n",
       "\n",
       "                       Authors  One Star Reviews  Two Star Reviews  \\\n",
       "0              Suzanne Collins             66715            127936   \n",
       "1  J.K. Rowling, Mary GrandPr√©             75504            101676   \n",
       "2              Stephenie Meyer            456191            436802   \n",
       "3                   Harper Lee             60427            117415   \n",
       "4          F. Scott Fitzgerald             86236            197621   \n",
       "\n",
       "   Three Star Reviews  Four Star Reviews  Five Star Reviews  \n",
       "0              560092            1481305            2706317  \n",
       "1              455024            1156318            3011543  \n",
       "2              793319             875073            1355439  \n",
       "3              446835            1001952            1714267  \n",
       "4              606158             936012             947718  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename the headers to be more explanatory\n",
    "new_book_df = new_book_df.rename(columns={\"isbn\": \"ISBN\",\n",
    "                                        \"original_title\": \"Original Title\",\n",
    "                                        \"original_publication_year\": \"Publication Year\",\n",
    "                                        \"authors\": \"Authors\",\n",
    "                                        \"ratings_1\": \"One Star Reviews\",\n",
    "                                        \"ratings_2\": \"Two Star Reviews\",\n",
    "                                        \"ratings_3\": \"Three Star Reviews\",\n",
    "                                        \"ratings_4\": \"Four Star Reviews\",\n",
    "                                        \"ratings_5\": \"Five Star Reviews\", })\n",
    "new_book_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push the remade DataFrame to a new CSV file\n",
    "new_book_df.to_csv(\"output/newBooks.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File to Load\n",
    "file_path = \"output/newBooks.csv\"\n",
    "\n",
    "# Import the books.csv file as a DataFrame\n",
    "new_books_df = pd.read_csv(file_path, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Publication Year</th>\n",
       "      <th>Original Title</th>\n",
       "      <th>Authors</th>\n",
       "      <th>One Star Reviews</th>\n",
       "      <th>Two Star Reviews</th>\n",
       "      <th>Three Star Reviews</th>\n",
       "      <th>Four Star Reviews</th>\n",
       "      <th>Five Star Reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>439023483</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>The Hunger Games</td>\n",
       "      <td>Suzanne Collins</td>\n",
       "      <td>66715</td>\n",
       "      <td>127936</td>\n",
       "      <td>560092</td>\n",
       "      <td>1481305</td>\n",
       "      <td>2706317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>439554934</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>Harry Potter and the Philosopher's Stone</td>\n",
       "      <td>J.K. Rowling, Mary GrandPr√©</td>\n",
       "      <td>75504</td>\n",
       "      <td>101676</td>\n",
       "      <td>455024</td>\n",
       "      <td>1156318</td>\n",
       "      <td>3011543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>316015849</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>Twilight</td>\n",
       "      <td>Stephenie Meyer</td>\n",
       "      <td>456191</td>\n",
       "      <td>436802</td>\n",
       "      <td>793319</td>\n",
       "      <td>875073</td>\n",
       "      <td>1355439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61120081</td>\n",
       "      <td>1960.0</td>\n",
       "      <td>To Kill a Mockingbird</td>\n",
       "      <td>Harper Lee</td>\n",
       "      <td>60427</td>\n",
       "      <td>117415</td>\n",
       "      <td>446835</td>\n",
       "      <td>1001952</td>\n",
       "      <td>1714267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>743273567</td>\n",
       "      <td>1925.0</td>\n",
       "      <td>The Great Gatsby</td>\n",
       "      <td>F. Scott Fitzgerald</td>\n",
       "      <td>86236</td>\n",
       "      <td>197621</td>\n",
       "      <td>606158</td>\n",
       "      <td>936012</td>\n",
       "      <td>947718</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ISBN  Publication Year                            Original Title  \\\n",
       "0  439023483            2008.0                          The Hunger Games   \n",
       "1  439554934            1997.0  Harry Potter and the Philosopher's Stone   \n",
       "2  316015849            2005.0                                  Twilight   \n",
       "3   61120081            1960.0                     To Kill a Mockingbird   \n",
       "4  743273567            1925.0                          The Great Gatsby   \n",
       "\n",
       "                       Authors  One Star Reviews  Two Star Reviews  \\\n",
       "0              Suzanne Collins             66715            127936   \n",
       "1  J.K. Rowling, Mary GrandPr√©             75504            101676   \n",
       "2              Stephenie Meyer            456191            436802   \n",
       "3                   Harper Lee             60427            117415   \n",
       "4          F. Scott Fitzgerald             86236            197621   \n",
       "\n",
       "   Three Star Reviews  Four Star Reviews  Five Star Reviews  \n",
       "0              560092            1481305            2706317  \n",
       "1              455024            1156318            3011543  \n",
       "2              793319             875073            1355439  \n",
       "3              446835            1001952            1714267  \n",
       "4              606158             936012             947718  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the modified GoodReads csv and store into Pandas DataFrame\n",
    "new_books_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4664"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the number of unique authors in the DataFrame\n",
    "author_count = len(new_book_df[\"Authors\"].unique())\n",
    "author_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1750.0\n",
      "2017.0\n"
     ]
    }
   ],
   "source": [
    "# Calculate the earliest/latest year a book was published\n",
    "earliest_year = new_book_df[\"Publication Year\"].min()\n",
    "latest_year = new_book_df[\"Publication Year\"].max()\n",
    "print(earliest_year)\n",
    "print(latest_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "796530182"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the total reviews for the entire dataset\n",
    "total_reviews = new_book_df[\"One Star Reviews\"].sum(\n",
    ") + new_book_df[\"Two Star Reviews\"].sum(\n",
    ") + new_book_df[\"Three Star Reviews\"].sum() + new_book_df[ \"Four Star Reviews\"].sum(\n",
    ") + new_book_df[\"Four Star Reviews\"].sum(\n",
    ") + new_book_df[\"Five Star Reviews\"].sum()\n",
    "\n",
    "total_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total Unique Authors</th>\n",
       "      <th>Earliest Year</th>\n",
       "      <th>Latest Year</th>\n",
       "      <th>Total Reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4664</td>\n",
       "      <td>-1750.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>796530182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Total Unique Authors  Earliest Year  Latest Year  Total Reviews\n",
       "0                  4664        -1750.0       2017.0      796530182"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Place all of the data found into a summary DataFrame\n",
    "\n",
    "summarized_df = pd.DataFrame({\"Total Unique Authors\": author_count,\n",
    "                              \"Earliest Year\": [earliest_year],\n",
    "                              \"Latest Year\": [latest_year],\n",
    "                              \"Total Reviews\": [total_reviews]})\n",
    "summarized_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><strong>Activity 05 Solution ‚úÖ</strong></summary>\n",
    "    \n",
    "```python\n",
    "# Import Dependencies\n",
    "import pandas as pd\n",
    "\n",
    "# Make a reference to the books.csv file path\n",
    "csv_path = \"Resources/books.csv\"\n",
    "\n",
    "# Import the books.csv file as a DataFrame\n",
    "books_df = pd.read_csv(csv_path, encoding=\"utf-8\")\n",
    "books_df.head()\n",
    "\n",
    "\n",
    "# Remove unecessary columns from the DataFrame and save the new DataFrame\n",
    "# Only keep: \"isbn\", \"original_publication_year\", \"original_title\", \"authors\",\n",
    "# \"ratings_1\", \"ratings_2\", \"ratings_3\", \"ratings_4\", \"ratings_5\"\n",
    "reduced_df = books_df[[\"isbn\", \"original_publication_year\", \"original_title\", \"authors\",\n",
    "                       \"ratings_1\", \"ratings_2\", \"ratings_3\", \"ratings_4\", \"ratings_5\"]]\n",
    "reduced_df.head()\n",
    "\n",
    "\n",
    "# Rename the headers to be more explanatory\n",
    "renamed_df = reduced_df.rename(columns={\"isbn\": \"ISBN\",\n",
    "                                        \"original_title\": \"Original Title\",\n",
    "                                        \"original_publication_year\": \"Publication Year\",\n",
    "                                        \"authors\": \"Authors\",\n",
    "                                        \"ratings_1\": \"One Star Reviews\",\n",
    "                                        \"ratings_2\": \"Two Star Reviews\",\n",
    "                                        \"ratings_3\": \"Three Star Reviews\",\n",
    "                                        \"ratings_4\": \"Four Star Reviews\",\n",
    "                                        \"ratings_5\": \"Five Star Reviews\", })\n",
    "renamed_df.head()\n",
    "\n",
    "# Push the remade DataFrame to a new CSV file\n",
    "renamed_df.to_csv(\"Output/books_clean.csv\",\n",
    "                  encoding=\"utf-8\", index=False, header=True)\n",
    "\n",
    "\n",
    "# File to Load\n",
    "goodreads_path = \"Output/books_clean.csv\"\n",
    "\n",
    "# Read the modified GoodReads csv and store into Pandas DataFrame\n",
    "goodreads_df = pd.read_csv(goodreads_path, encoding=\"utf-8\")\n",
    "goodreads_df.head()\n",
    "\n",
    "\n",
    "# Calculate the number of unique authors in the DataFrame\n",
    "author_count = len(goodreads_df[\"Authors\"].unique())\n",
    "\n",
    "# Calculate the earliest/latest year a book was published\n",
    "earliest_year = goodreads_df[\"Publication Year\"].min()\n",
    "latest_year = goodreads_df[\"Publication Year\"].max()\n",
    "\n",
    "# Calculate the total reviews for the entire dataset\n",
    "total_reviews = goodreads_df[\"One Star Reviews\"].sum() + goodreads_df[\"Two Star Reviews\"].sum(\n",
    ") + goodreads_df[\"Three Star Reviews\"].sum() + goodreads_df[\"Four Star Reviews\"].sum() + goodreads_df[\"Five Star Reviews\"].sum()\n",
    "\n",
    "# Place all of the data found into a summary DataFrame\n",
    "summary_table = pd.DataFrame({\"Total Unique Authors\": author_count,\n",
    "                              \"Earliest Year\": [earliest_year],\n",
    "                              \"Latest Year\": [latest_year],\n",
    "                              \"Total Reviews\": [total_reviews]})\n",
    "summary_table\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructor Turn - 06 - Merging  - üë©‚Äçüè´üßë‚Äçüè´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>name</th>\n",
       "      <th>email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>112</td>\n",
       "      <td>John</td>\n",
       "      <td>jman@gmail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>403</td>\n",
       "      <td>Kelly</td>\n",
       "      <td>kelly@aol.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>999</td>\n",
       "      <td>Sam</td>\n",
       "      <td>sports@school.edu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>543</td>\n",
       "      <td>April</td>\n",
       "      <td>April@yahoo.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>123</td>\n",
       "      <td>Bobbo</td>\n",
       "      <td>HeyImBobbo@msn.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id   name               email\n",
       "0          112   John          jman@gmail\n",
       "1          403  Kelly       kelly@aol.com\n",
       "2          999    Sam   sports@school.edu\n",
       "3          543  April     April@yahoo.com\n",
       "4          123  Bobbo  HeyImBobbo@msn.com"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dependencies\n",
    "import pandas as pd\n",
    "\n",
    "raw_data_info = {\n",
    "    \"customer_id\": [112, 403, 999, 543, 123],\n",
    "    \"name\": [\"John\", \"Kelly\", \"Sam\", \"April\", \"Bobbo\"],\n",
    "    \"email\": [\"jman@gmail\", \"kelly@aol.com\", \"sports@school.edu\", \"April@yahoo.com\", \"HeyImBobbo@msn.com\"]\n",
    "}\n",
    "info_pd = pd.DataFrame(raw_data_info, columns=[\"customer_id\", \"name\", \"email\"])\n",
    "info_pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create DataFrames\n",
    "raw_data_items = {\n",
    "    \"customer_id\": [403, 112, 543, 999, 654],\n",
    "    \"item\": [\"soda\", \"chips\", \"TV\", \"Laptop\", \"Cooler\"],\n",
    "    \"cost\": [3.00, 4.50, 600, 900, 150]\n",
    "}\n",
    "items_pd = pd.DataFrame(raw_data_items, columns=[\n",
    "                        \"customer_id\", \"item\", \"cost\"])\n",
    "items_pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge two dataframes using an inner join\n",
    "merge_table = pd.merge(info_pd, items_pd, on=\"customer_id\")\n",
    "merge_table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Merge two dataframes using an outer join\n",
    "merge_table = pd.merge(info_pd, items_pd, on=\"customer_id\", how=\"outer\")\n",
    "merge_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Merge two dataframes using a left join\n",
    "merge_table = pd.merge(info_pd, items_pd, on=\"customer_id\", how=\"left\")\n",
    "merge_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Merge two dataframes using a right join\n",
    "merge_table = pd.merge(info_pd, items_pd, on=\"customer_id\", how=\"right\")\n",
    "merge_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructor Turn - 07 - Mapping - üë©‚Äçüè´üßë‚Äçüè´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Mapping lets you format an entire DataFrame\n",
    "file = \"Resources/sample_data.csv\"\n",
    "file_df = pd.read_csv(file)\n",
    "file_df.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Map to format all the columns\n",
    "file_df[\"avg_cost\"] = file_df[\"avg_cost\"].map(\"${:.2f}\".format)\n",
    "file_df[\"population\"] = file_df[\"population\"].map(\"{:,}\".format)\n",
    "file_df[\"other\"] = file_df[\"other\"].map(\"{:.2f}\".format)\n",
    "file_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping has changed the datatypes of the columns to strings\n",
    "file_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Students Turn - 08 - Format Mapping - üë©‚Äçüéìüë®‚Äçüéì\n",
    "# Format Mappping\n",
    "\n",
    "## Instructions\n",
    "\n",
    "* Read in the `sales_data.csv` using Pandas.\n",
    "\n",
    "* Get the data types of each column.\n",
    "\n",
    "* Get a list of all columns within the DataFrame.\n",
    "\n",
    "* Use the `map()` and `.format` methods to format the following columns:\n",
    "    * `Units Sold` with comma notation\n",
    "    * `Unit Price`, `Unit Cost`, `Total Revenue`, `Total Cost`, and `Total Profit` with a dollar sign, comma notation, and to two decimal places.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Mapping lets you format an entire DataFrame\n",
    "file = \"Resources/sales_data.csv\"\n",
    "sales_data_df = pd.read_csv(file)\n",
    "sales_data_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use map() to format:\n",
    "# Units Sold with comma notation\n",
    "# Unit Price, Unit Cost, Total Revenue, Total Cost, and Total\n",
    "# with a dollar sign, comma notation, and to two decimal places.\n",
    "\n",
    "\n",
    "# Mapping has changed the datatypes of the columns to strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><strong>Activity 08 Solution ‚úÖ</strong></summary>\n",
    "    \n",
    "```python\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Mapping lets you format an entire DataFrame\n",
    "file = \"Resources/sales_data.csv\"\n",
    "sales_data_df = pd.read_csv(file)\n",
    "sales_data_df.head()\n",
    "\n",
    "sales_data_df.dtypes\n",
    "\n",
    "sales_data_df.columns\n",
    "\n",
    "# Use map() to format:\n",
    "# Units Sold with comma notation\n",
    "# Unit Price, Unit Cost, Total Revenue, Total Cost, and Total\n",
    "# with a dollar sign, comma notation, and to two decimal places.\n",
    "sales_data_df[\"Units Sold\"] = sales_data_df[\"Units Sold\"].map(\"{:,.0f}\".format)\n",
    "sales_data_df[\"Unit Price\"] = sales_data_df[\"Unit Price\"].map(\"${:,.2f}\".format)\n",
    "sales_data_df[\"Unit Cost\"] = sales_data_df[\"Unit Cost\"].map(\"${:,.2f}\".format)\n",
    "sales_data_df[\"Total Revenue\"] = sales_data_df[\"Total Revenue\"].map(\"${:,.2f}\".format)\n",
    "sales_data_df[\"Total Cost\"] = sales_data_df[\"Total Cost\"].map(\"${:,.2f}\".format)\n",
    "sales_data_df[\"Total Profit\"] = sales_data_df[\"Total Profit\"].map(\"${:,.2f}\".format)\n",
    "sales_data_df.head()\n",
    "\n",
    "# Mapping has changed the datatypes of the columns to strings\n",
    "sales_data_df.dtypes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "</details>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
